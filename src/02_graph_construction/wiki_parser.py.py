# íŒŒì¼ëª…: src/02_graph_construction/wiki_parser.py
import os
import pandas as pd
import html
import re
from bs4 import BeautifulSoup

# ==========================================
# 1. ê²½ë¡œ ì„¤ì •
# ==========================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(os.path.dirname(BASE_DIR))

# ì…ë ¥: ì²­ì†Œëœ XML (Clean Version)
SOURCE_DIR = os.path.join(ROOT_DIR, 'data', '04_clean_xml')
# ì¶œë ¥: v3.0 ê·¸ë˜í”„ CSV
OUTPUT_DIR = os.path.join(ROOT_DIR, 'data', '05_graph_csv')

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ==========================================
# 2. v3.0 ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ë¡œì§
# ==========================================
def analyze_entity_v3(tag, text_value, context_hint=""):
    """
    XML íƒœê·¸ì™€ ë¬¸ë§¥ì„ ë¶„ì„í•˜ì—¬ v3.0 ê¸°ì¤€ì˜ Classì™€ Attributeë¥¼ ë„ì¶œ
    Returns: (Class, Attribute_Name, Attribute_Value)
    """
    # 1. Actor (ì¸ë¬¼/ê¸°ê´€)
    if "Participant" in tag or "Architect" in tag or "Builder" in tag:
        role = "Unspecified"
        if any(k in context_hint for k in ["ì„¤ê³„", "ê±´ì¶•ì‚¬", "ê¸°ì‚¬"]): role = "Architect (ì„¤ê³„)"
        elif any(k in context_hint for k in ["ì‹œê³µ", "ì²­ë¶€", "ê³µì‚¬", "ì¡°", "ì‘ì—…"]): role = "Builder (ì‹œê³µ)"
        elif any(k in context_hint for k in ["ë‚©í’ˆ", "ìƒì ", "ìƒíšŒ"]): role = "Supplier (ë‚©í’ˆ)"
        elif any(k in context_hint for k in ["ê°ë¦¬", "ê°ë…"]): role = "Supervisor (ê°ë¦¬)"
        return "Actor", "Role", role

    # 2. Structure (êµ¬ì¡°)
    if "StructuralSystem" in tag or "BuildingElement" in tag:
        st_type = "Structure Element"
        if any(k in context_hint for k in ["ì² ê·¼", "RC", "ì½˜í¬ë¦¬íŠ¸"]): st_type = "RC"
        elif any(k in context_hint for k in ["ë²½ëŒ", "ì¡°ì "]): st_type = "Masonry"
        elif any(k in context_hint for k in ["ëª©ì¡°", "ì§€ë¶•"]): st_type = "Timber/Roof"
        return "Structure", "Type", st_type

    # 3. Material (ì¬ë£Œ)
    if "Material" in tag or "Covering" in tag or "Finish" in tag:
        return "Material", "Name", text_value

    # 4. Facility (ì„¤ë¹„)
    if "brick" in tag or "Heating" in tag or "Plumbing" in tag or "Lighting" in tag or "Equipment" in tag:
        fac_type = "General Facility"
        if any(k in tag for k in ["Heating", "Heat"]): fac_type = "Heating (ë‚œë°©)"
        elif any(k in tag for k in ["Plumbing", "Water", "Sanitary"]): fac_type = "Plumbing (ìœ„ìƒ)"
        elif any(k in tag for k in ["Lighting", "Elec", "Power"]): fac_type = "Electrical (ì „ê¸°)"
        elif any(k in tag for k in ["Elevator", "Transport"]): fac_type = "Transport (ìŠ¹ê°•ê¸°)"
        return "Facility", "Type", fac_type

    # 5. Location (ìœ„ì¹˜)
    if "isLocatedIn" in tag or "Address" in tag:
        return "Location", "AddressOld", text_value

    # 6. Year (ì—°ë„/ì‹œê¸°)
    if "TimeSpan" in tag or "Date" in tag or "Year" in tag:
        attr_name = "EventDate"
        if "ì°©ê³µ" in context_hint: attr_name = "StartDate"
        elif "ì¤€ê³µ" in context_hint: attr_name = "EndDate"
        return "Year", attr_name, text_value

    # 7. Building Attributes (ê±´ë¬¼ ìì²´ ì†ì„±)
    # ë©´ì 
    if "Area" in tag or any(unit in text_value for unit in ["í‰", "m2", "ã¡"]):
        attr_name = "TotalArea"
        if "ëŒ€ì§€" in context_hint or "ë¶€ì§€" in context_hint: attr_name = "SiteArea"
        return "Building", attr_name, text_value
    # ë†’ì´
    if "Height" in tag or any(unit in text_value for unit in ["ì²™", "m", "ë¯¸í„°"]):
        return "Building", "Height", text_value
    # ì¸µìˆ˜
    if "Storey" in tag or "Floors" in tag:
        return "Building", "Floors", text_value
    # ìš©ë„
    if "Function" in tag or "Use" in tag:
        return "Building", "Function", text_value

    # ë§¤í•‘ë˜ì§€ ì•ŠëŠ” ê¸°íƒ€ íƒœê·¸
    return "Etc", "Description", text_value

# ==========================================
# 3. íŒŒì‹± ì—”ì§„
# ==========================================
def extract_text_from_xml(xml_content):
    try:
        soup = BeautifulSoup(xml_content, "html.parser")
        text_tag = soup.find("text")
        if text_tag:
            return html.unescape(text_tag.get_text())
        else:
            return html.unescape(xml_content)
    except:
        return xml_content

def parse_wiki_text_v3(doc_name, wiki_source):
    rows = []
    
    # ê¸°ë³¸ í–‰: ê±´ë¬¼ ìì²´ ì •ì˜
    rows.append({
        "Source_Document": doc_name,
        "Class": "Building",
        "Entity_Name": doc_name, # ê±´ë¬¼ëª…ì´ ê³§ Entity
        "Attribute_Type": "Name",
        "Attribute_Value": doc_name,
        "Original_Context": "Document Title"
    })

    lines = wiki_source.split('\n')
    for line in lines:
        line_soup = BeautifulSoup(line, "html.parser")
        current_context = line_soup.get_text().strip()
        
        spans = line_soup.find_all("span")
        for span in spans:
            if span.has_attr("title"):
                tag = span["title"]
                val = span.get_text().strip()
                
                # v3.0 ë¶„ì„ ì‹¤í–‰
                cls, attr_type, attr_val = analyze_entity_v3(tag, val, context_hint=current_context)
                
                # ë°ì´í„° í–‰ ì¶”ê°€
                rows.append({
                    "Source_Document": doc_name,
                    "Class": cls,             # Actor, Material, Facility ...
                    "Entity_Name": val,       # ì‹¤ì œ í…ìŠ¤íŠ¸ (ì˜ˆ: ë‹¤ì „ìˆœì‚¼ë‘, ëŒ€ë¦¬ì„)
                    "Attribute_Type": attr_type, # Role, Type, Origin ...
                    "Attribute_Value": attr_val, # Builder, RC, Heating ...
                    "Original_Context": current_context[:100] # ê²€ì¦ìš© ë¬¸ë§¥ (ë„ˆë¬´ ê¸¸ë©´ ìë¦„)
                })
    return rows

# ==========================================
# 4. ì‹¤í–‰ (Batch)
# ==========================================
def run_batch_conversion():
    if not os.path.exists(SOURCE_DIR):
        print(f"âŒ Error: ì…ë ¥ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤ -> {SOURCE_DIR}")
        return

    xml_files = [f for f in os.listdir(SOURCE_DIR) if f.endswith('.xml')]
    print(f"ğŸš€ [KMUA v3.0] ì´ {len(xml_files)}ê°œì˜ XMLì„ ë¶„ì„í•©ë‹ˆë‹¤...")

    for filename in xml_files:
        doc_name = os.path.splitext(filename)[0].replace(" ", "_")
        file_path = os.path.join(SOURCE_DIR, filename)
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = extract_text_from_xml(f.read())
            
            # v3 íŒŒì‹±
            parsed_rows = parse_wiki_text_v3(doc_name, content)
            df = pd.DataFrame(parsed_rows)
            
            # v3ë¶€í„°ëŠ” 'Label' ëŒ€ì‹  'Class'ì™€ 'Attribute' ì»¬ëŸ¼ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì €ì¥
            output_csv = f"kmua_v3_{doc_name}.csv"
            output_path = os.path.join(OUTPUT_DIR, output_csv)
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            
            print(f"  âœ… ë³€í™˜ ì™„ë£Œ: {filename} -> {output_csv}")
            
        except Exception as e:
            print(f"  âŒ ì‹¤íŒ¨ ({filename}): {e}")

    print("\nğŸ‰ ëª¨ë“  ë³€í™˜ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! (v3.0 Schema Applied)")

if __name__ == "__main__":
    run_batch_conversion()